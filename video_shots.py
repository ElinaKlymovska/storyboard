"""CLI —É—Ç–∏–ª—ñ—Ç–∞ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–∫—Ä—ñ–Ω—à–æ—Ç—ñ–≤ –∑ –≤—ñ–¥–µ–æ —á–µ—Ä–µ–∑ –∑–∞–¥–∞–Ω–∏–π —ñ–Ω—Ç–µ—Ä–≤–∞–ª."""

from __future__ import annotations

import argparse
import math
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Sequence, Tuple
import base64
import io

import cv2
import numpy as np
from PIL import Image
import replicate
from notion_client import Client


CYRILLIC_S_LOWER = "—Å"
CYRILLIC_S_UPPER = "–°"


class VideoShotsError(Exception):
    """–ë–∞–∑–æ–≤–µ –≤–∏–∫–ª—é—á–µ–Ω–Ω—è –¥–ª—è —É—Ç–∏–ª—ñ—Ç–∏."""


@dataclass(frozen=True)
class TimePoint:
    index: int
    seconds: float
    frame_index: int


def parse_time_value(value: str, *, allow_zero: bool = False) -> float:
    """–ü–æ–≤–µ—Ä—Ç–∞—î —á–∞—Å —É —Å–µ–∫—É–Ω–¥–∞—Ö –∑ —Ä—è–¥–∫–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞."""

    if value is None:
        raise VideoShotsError("–ù–µ –≤–∫–∞–∑–∞–Ω–æ –∑–Ω–∞—á–µ–Ω–Ω—è —á–∞—Å—É")

    normalized = (
        value.strip()
        .replace(" ", "")
        .replace(",", ".")
        .replace(CYRILLIC_S_LOWER, "s")
        .replace(CYRILLIC_S_UPPER, "s")
        .lower()
    )

    if not normalized:
        raise VideoShotsError("–ü–æ—Ä–æ–∂–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è —á–∞—Å—É")

    if ":" in normalized:
        parts = normalized.split(":")
        if len(parts) != 3:
            raise VideoShotsError(
                f"–ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —á–∞—Å—É '{value}'. –û—á—ñ–∫—É—î—Ç—å—Å—è HH:MM:SS.mmm"
            )
        hours_str, minutes_str, seconds_str = parts
        try:
            hours = int(hours_str)
            minutes = int(minutes_str)
            seconds = float(seconds_str)
        except ValueError as exc:
            raise VideoShotsError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ —á–∞—Å '{value}'") from exc
        if minutes >= 60 or minutes < 0:
            raise VideoShotsError(f"–•–≤–∏–ª–∏–Ω–∏ –≤–∏—Ö–æ–¥—è—Ç—å –∑–∞ –º–µ–∂—ñ 0-59 —É –∑–Ω–∞—á–µ–Ω–Ω—ñ '{value}'")
        if seconds < 0 or seconds >= 60:
            raise VideoShotsError(f"–°–µ–∫—É–Ω–¥–∏ –≤–∏—Ö–æ–¥—è—Ç—å –∑–∞ –º–µ–∂—ñ 0-59 —É –∑–Ω–∞—á–µ–Ω–Ω—ñ '{value}'")
        total_seconds = hours * 3600 + minutes * 60 + seconds
    else:
        suffix = None
        if normalized.endswith("ms"):
            suffix = "ms"
            number_part = normalized[:-2]
        elif normalized.endswith("s"):
            suffix = "s"
            number_part = normalized[:-1]
        else:
            number_part = normalized

        if not number_part:
            raise VideoShotsError(f"–ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —á–∞—Å—É '{value}'")

        try:
            number = float(number_part)
        except ValueError as exc:
            raise VideoShotsError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ —á–∏—Å–ª–æ —É '{value}'") from exc

        if suffix == "ms":
            total_seconds = number / 1000.0
        else:
            total_seconds = number

    if total_seconds < 0 or (total_seconds == 0 and not allow_zero):
        raise VideoShotsError("–ß–∞—Å –º–∞—î –±—É—Ç–∏ > 0")

    return total_seconds


def format_timestamp(seconds: float, *, precision: str) -> str:
    include_ms = precision == "ms"
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60

    if include_ms:
        sec_str = f"{secs:06.3f}" if hours else f"{secs:06.3f}"
    else:
        secs = int(round(secs))
        sec_str = f"{secs:02d}"

    if hours:
        if include_ms:
            return f"{hours:02d}:{minutes:02d}:{sec_str}"
        return f"{hours:02d}:{minutes:02d}:{sec_str}"
    return f"{minutes:02d}:{sec_str}"


def format_timestamp_for_name(seconds: float, *, precision: str) -> str:
    include_ms = precision == "ms"
    minutes_total = int(seconds // 60)
    secs_float = seconds % 60
    hours, minutes = divmod(minutes_total, 60)
    secs = int(secs_float)
    millis = int(round((secs_float - secs) * 1000))

    parts = []
    if hours:
        parts.append(f"{hours:02d}")
    parts.append(f"{minutes:02d}")
    parts.append(f"{secs:02d}")
    if include_ms:
        parts.append(f"{millis:03d}")
    return "-".join(parts)


def collect_timepoints(
    start: float,
    end: float,
    step: float,
    fps: float,
    frame_count: int,
) -> List[TimePoint]:
    if step <= 0:
        raise VideoShotsError("–Ü–Ω—Ç–µ—Ä–≤–∞–ª –º–∞—î –±—É—Ç–∏ > 0")

    if end < start:
        raise VideoShotsError("–ü–∞—Ä–∞–º–µ—Ç—Ä --end –º–∞—î –±—É—Ç–∏ >= --start")

    duration = frame_count / fps if fps > 0 else 0
    hard_end = min(end, duration)
    if hard_end < start:
        raise VideoShotsError("–°—Ç–∞—Ä—Ç–æ–≤–∏–π —á–∞—Å –ø–µ—Ä–µ–≤–∏—â—É—î —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –≤—ñ–¥–µ–æ")

    timepoints: List[TimePoint] = []
    index = 0
    max_iterations = 10_000_000
    epsilon = step / 10_000 if step else 1e-9

    while True:
        current_time = start + index * step
        if current_time > hard_end + epsilon:
            break

        frame_index = int(round(current_time * fps))
        frame_index = max(0, min(frame_index, frame_count - 1))

        timepoints.append(TimePoint(index=len(timepoints) + 1, seconds=current_time, frame_index=frame_index))

        index += 1
        if index > max_iterations:
            raise VideoShotsError(
                "–ó–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —ñ–Ω—Ç–µ—Ä–≤–∞–ª –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ --end/--start"
            )

    return timepoints


def apply_rotation(frame: np.ndarray, rotation: int) -> np.ndarray:
    if rotation == 0:
        return frame
    if rotation == 90:
        return cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
    if rotation == 180:
        return cv2.rotate(frame, cv2.ROTATE_180)
    if rotation == 270:
        return cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)
    raise VideoShotsError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π –∫—É—Ç –æ–±–µ—Ä—Ç–∞–Ω–Ω—è: {rotation}")


def render_label_panel(
    frame: np.ndarray,
    label_lines: Sequence[str],
    font=cv2.FONT_HERSHEY_SIMPLEX,
    font_scale: float = 0.8,
    font_thickness: int = 2,
    padding: int = 12,
    panel_bg_color: Tuple[int, int, int] = (0, 0, 0),
    panel_alpha: float = 0.6,
    text_color: Tuple[int, int, int] = (255, 255, 255),
) -> np.ndarray:
    height, width = frame.shape[:2]

    # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —à–∏—Ä–∏–Ω–∏ –ø–∞–Ω–µ–ª—ñ –∑–∞ –¥–æ–≤–∂–∏–Ω–æ—é —Ç–µ–∫—Å—Ç—É
    max_text_width = 0
    text_height_total = 0
    line_height = 0
    for line in label_lines:
        (text_width, text_height), baseline = cv2.getTextSize(line, font, font_scale, font_thickness)
        max_text_width = max(max_text_width, text_width)
        line_height = max(line_height, text_height + baseline)
        text_height_total += text_height + baseline

    panel_width = max_text_width + padding * 2
    if panel_width < int(width * 0.18):
        panel_width = int(width * 0.18)

    new_width = width + panel_width
    result = np.zeros((height, new_width, 3), dtype=np.uint8)
    result[:, :width] = frame

    overlay = result.copy()
    overlay[:, width:] = panel_bg_color
    cv2.addWeighted(overlay, panel_alpha, result, 1 - panel_alpha, 0, result)

    # –†–æ–∑–º—ñ—â—É—î–º–æ —Ç–µ–∫—Å—Ç —É –≤–µ—Ä—Ö–Ω—ñ–π —á–∞—Å—Ç–∏–Ω—ñ –ø–∞–Ω–µ–ª—ñ
    y = padding + line_height
    x = width + padding
    for line in label_lines:
        (text_width, text_height), baseline = cv2.getTextSize(line, font, font_scale, font_thickness)
        cv2.putText(result, line, (x, y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)
        y += line_height + padding // 2

    return result


def save_frame_image(
    frame: np.ndarray,
    output_path: Path,
    image_format: str,
) -> None:
    params = []
    if image_format == "jpg":
        params = [int(cv2.IMWRITE_JPEG_QUALITY), 95]

    success = cv2.imwrite(str(output_path), frame, params)
    if not success:
        raise VideoShotsError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ —Ñ–∞–π–ª '{output_path}'")


def combine_to_pdf(image_paths: Sequence[Path], analyses: List[Path], timepoints: List[TimePoint], pdf_path: Path) -> None:
    """–û–±'—î–¥–Ω—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –∞–Ω–∞–ª—ñ–∑–æ–º –≤ PDF —Ñ–∞–π–ª."""
    if not image_paths:
        return
    
    try:
        import json
        from reportlab.lib.pagesizes import letter, A4
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage, PageBreak
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.lib.units import inch
        from reportlab.lib import colors
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ PDF –¥–æ–∫—É–º–µ–Ω—Ç
        doc = SimpleDocTemplate(str(pdf_path), pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=18)
        story = []
        
        # –°—Ç–∏–ª—ñ
        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=16,
            spaceAfter=30,
            alignment=1,  # Center
            textColor=colors.darkblue
        )
        
        frame_style = ParagraphStyle(
            'FrameTitle',
            parent=styles['Heading2'],
            fontSize=14,
            spaceAfter=12,
            textColor=colors.darkblue
        )
        
        analysis_style = ParagraphStyle(
            'Analysis',
            parent=styles['Normal'],
            fontSize=10,
            spaceAfter=6,
            leftIndent=20,
            textColor=colors.darkgreen
        )
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        story.append(Paragraph("üé¨ AI-Generated Storyboard Analysis", title_style))
        story.append(Spacer(1, 12))
        
        # –î–æ–¥–∞—î–º–æ –∫–æ–∂–µ–Ω –∫–∞–¥—Ä –∑ –∞–Ω–∞–ª—ñ–∑–æ–º
        for i, (img_path, analysis_path, tp) in enumerate(zip(image_paths, analyses, timepoints), 1):
            time_label = format_timestamp(tp.seconds, precision='ms')
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–¥—Ä—É
            story.append(Paragraph(f"Frame {i} - {time_label}", frame_style))
            
            # –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            try:
                img = RLImage(str(img_path), width=6*inch, height=4*inch)
                story.append(img)
                story.append(Spacer(1, 12))
            except Exception as e:
                story.append(Paragraph(f"Error loading image: {e}", styles['Normal']))
            
            # –ê–Ω–∞–ª—ñ–∑ —è–∫—â–æ —î
            if analysis_path and analysis_path.exists():
                try:
                    with open(analysis_path, 'r', encoding='utf-8') as f:
                        analysis_data = json.load(f)
                    
                    # Scene Description
                    scene_desc = analysis_data.get('scene_description', 'No description available')
                    story.append(Paragraph(f"<b>üìù Scene Description:</b>", analysis_style))
                    story.append(Paragraph(scene_desc, analysis_style))
                    story.append(Spacer(1, 6))
                    
                    # Visual Elements
                    if 'visual_elements' in analysis_data:
                        ve = analysis_data['visual_elements']
                        story.append(Paragraph(f"<b>üëÅÔ∏è Visual Elements:</b>", analysis_style))
                        if ve.get('characters'):
                            story.append(Paragraph(f"Characters: {', '.join(ve['characters'])}", analysis_style))
                        if ve.get('objects'):
                            story.append(Paragraph(f"Objects: {', '.join(ve['objects'])}", analysis_style))
                        if ve.get('background'):
                            story.append(Paragraph(f"Background: {ve['background']}", analysis_style))
                        if ve.get('lighting'):
                            story.append(Paragraph(f"Lighting: {ve['lighting']}", analysis_style))
                        story.append(Spacer(1, 6))
                    
                    # Composition
                    if 'composition' in analysis_data:
                        comp = analysis_data['composition']
                        story.append(Paragraph(f"<b>üé¨ Composition:</b>", analysis_style))
                        if comp.get('shot_type'):
                            story.append(Paragraph(f"Shot Type: {comp['shot_type']}", analysis_style))
                        if comp.get('camera_angle'):
                            story.append(Paragraph(f"Camera Angle: {comp['camera_angle']}", analysis_style))
                        if comp.get('framing'):
                            story.append(Paragraph(f"Framing: {comp['framing']}", analysis_style))
                        story.append(Spacer(1, 6))
                    
                    # Storyboard Suitability
                    if 'storyboard_suitability' in analysis_data:
                        ss = analysis_data['storyboard_suitability']
                        score = ss.get('score', 0)
                        score_color = colors.green if score >= 7 else colors.orange if score >= 4 else colors.red
                        story.append(Paragraph(f"<b>‚≠ê Storyboard Suitability: <font color='{score_color}'>{score}/10</font></b>", analysis_style))
                        if ss.get('reasoning'):
                            story.append(Paragraph(f"Reasoning: {ss['reasoning']}", analysis_style))
                        if ss.get('key_moments'):
                            story.append(Paragraph(f"Key Moments: {', '.join(ss['key_moments'])}", analysis_style))
                        if ss.get('transition_potential'):
                            story.append(Paragraph(f"Transition Potential: {ss['transition_potential']}", analysis_style))
                        story.append(Spacer(1, 6))
                    
                    # Production Notes
                    if 'production_notes' in analysis_data:
                        pn = analysis_data['production_notes']
                        story.append(Paragraph(f"<b>üé≠ Production Notes:</b>", analysis_style))
                        if pn.get('emotions'):
                            story.append(Paragraph(f"Emotions: {', '.join(pn['emotions'])}", analysis_style))
                        if pn.get('action_level'):
                            story.append(Paragraph(f"Action Level: {pn['action_level']}", analysis_style))
                        if pn.get('makeup_visible') is not None:
                            story.append(Paragraph(f"Makeup Visible: {pn['makeup_visible']}", analysis_style))
                        if pn.get('technical_quality'):
                            story.append(Paragraph(f"Technical Quality: {pn['technical_quality']}", analysis_style))
                    
                except Exception as e:
                    story.append(Paragraph(f"Error reading analysis: {e}", styles['Normal']))
            else:
                story.append(Paragraph("No analysis available for this frame.", analysis_style))
            
            # –†–æ–∑–¥—ñ–ª—é–≤–∞—á –º—ñ–∂ –∫–∞–¥—Ä–∞–º–∏ (–∫—Ä—ñ–º –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ)
            if i < len(image_paths):
                story.append(PageBreak())
        
        # –ì–µ–Ω–µ—Ä—É—î–º–æ PDF
        doc.build(story)
        
    except ImportError:
        # Fallback –¥–æ –ø—Ä–æ—Å—Ç–æ–≥–æ PDF —è–∫—â–æ reportlab –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ
        images = []
        for idx, path in enumerate(image_paths):
            img = Image.open(path)
            if img.mode != "RGB":
                img = img.convert("RGB")
            images.append(img)

        head, *tail = images
        head.save(pdf_path, "PDF", resolution=100.0, save_all=True, append_images=tail)


def parse_audio_transcript(audio_file_path: Path) -> dict:
    """–ü–∞—Ä—Å–∏—Ç—å –∞—É–¥—ñ–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Å–ª–æ–≤–Ω–∏–∫ –∑ —Ç–∞–π–º–∫–æ–¥–∞–º–∏."""
    try:
        if not audio_file_path.exists():
            return {}
        
        with open(audio_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –ü–∞—Ä—Å–∏–º–æ —Ç–∞–π–º–∫–æ–¥–∏ —Ç–∞ —Ç–µ–∫—Å—Ç
        import re
        pattern = r'\[(\d{2}:\d{2}:\d{2}:\d{2}) - (\d{2}:\d{2}:\d{2}:\d{2})\]\s*(.+)'
        matches = re.findall(pattern, content)
        
        transcript_data = {}
        for start_time, end_time, text in matches:
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ç–∞–π–º–∫–æ–¥ –≤ —Å–µ–∫—É–Ω–¥–∏
            start_seconds = parse_timecode_to_seconds(start_time)
            end_seconds = parse_timecode_to_seconds(end_time)
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–∞–¥—Ä—É
            transcript_data[start_seconds] = {
                'start': start_time,
                'end': end_time,
                'text': text.strip()
            }
        
        return transcript_data
        
    except Exception as exc:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É: {exc}", file=sys.stderr)
        return {}


def parse_timecode_to_seconds(timecode: str) -> float:
    """–ö–æ–Ω–≤–µ—Ä—Ç—É—î —Ç–∞–π–º–∫–æ–¥ HH:MM:SS:FF –≤ —Å–µ–∫—É–Ω–¥–∏."""
    try:
        parts = timecode.split(':')
        hours = int(parts[0])
        minutes = int(parts[1])
        seconds = int(parts[2])
        frames = int(parts[3])
        
        # –ü—Ä–∏–ø—É—Å–∫–∞—î–º–æ 30 FPS
        total_seconds = hours * 3600 + minutes * 60 + seconds + frames / 30.0
        return total_seconds
    except:
        return 0.0


def get_vo_for_timestamp(transcript_data: dict, timestamp: float) -> str:
    """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å VO/Sound –¥–ª—è –∑–∞–¥–∞–Ω–æ–≥–æ —Ç–∞–π–º–∫–æ–¥—É."""
    if not transcript_data:
        return ""
    
    # –®—É–∫–∞—î–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π —Ç–∞–π–º–∫–æ–¥
    closest_time = None
    min_diff = float('inf')
    
    for time_key in transcript_data.keys():
        diff = abs(time_key - timestamp)
        if diff < min_diff:
            min_diff = diff
            closest_time = time_key
    
    # –Ø–∫—â–æ –∑–Ω–∞–π—à–ª–∏ –±–ª–∏–∑—å–∫–∏–π —Ç–∞–π–º–∫–æ–¥ (–≤ –º–µ–∂–∞—Ö 2 —Å–µ–∫—É–Ω–¥)
    if closest_time is not None and min_diff <= 2.0:
        return transcript_data[closest_time]['text']
    
    return ""


def create_structured_analysis_from_text(text: str) -> dict:
    """–°—Ç–≤–æ—Ä—é—î —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å—É."""
    import re
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ç–∏–ø –∫–∞–¥—Ä—É
    shot_type = "close-up"
    if "wide" in text.lower() or "full" in text.lower():
        shot_type = "wide"
    elif "medium" in text.lower():
        shot_type = "medium"
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –µ–º–æ—Ü—ñ—ó
    emotions = []
    if any(word in text.lower() for word in ["happy", "smiling", "joy", "excited"]):
        emotions.append("happy")
    if any(word in text.lower() for word in ["serious", "focused", "concentrated"]):
        emotions.append("serious")
    if any(word in text.lower() for word in ["relaxed", "calm", "peaceful"]):
        emotions.append("relaxed")
    if any(word in text.lower() for word in ["dramatic", "intense", "dramatic"]):
        emotions.append("dramatic")
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ä—ñ–≤–µ–Ω—å –¥—ñ—ó
    action_level = "static"
    if any(word in text.lower() for word in ["moving", "action", "motion", "dynamic"]):
        action_level = "medium"
    if any(word in text.lower() for word in ["fast", "quick", "rapid", "intense"]):
        action_level = "high"
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –æ—Ü—ñ–Ω–∫—É –ø—Ä–∏–¥–∞—Ç–Ω–æ—Å—Ç—ñ
    score = 5
    if "close-up" in text.lower() and "face" in text.lower():
        score = 7
    if "background" in text.lower() and "black" in text.lower():
        score += 1
    if emotions:
        score += 1
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø–µ—Ä—Å–æ–Ω–∞–∂—ñ–≤
    characters = ["person"]
    if "woman" in text.lower():
        characters = ["woman"]
    elif "man" in text.lower():
        characters = ["man"]
    
    return {
        "scene_description": text[:200] + "..." if len(text) > 200 else text,
        "visual_elements": {
            "characters": characters,
            "objects": ["face", "hair"] if "hair" in text.lower() else ["face"],
            "background": "black background" if "black" in text.lower() else "background",
            "lighting": "cinematic lighting" if "lighting" in text.lower() else "natural lighting"
        },
        "composition": {
            "shot_type": shot_type,
            "camera_angle": "eye-level",
            "framing": "face-focused framing"
        },
        "storyboard_suitability": {
            "score": min(score, 10),
            "reasoning": f"Good for storyboard: {shot_type} shot with clear character focus",
            "key_moments": ["character expression", "visual focus"],
            "transition_potential": "good" if score >= 6 else "fair"
        },
        "production_notes": {
            "makeup_visible": "makeup" in text.lower(),
            "emotions": emotions if emotions else ["neutral"],
            "action_level": action_level,
            "technical_quality": "good"
        }
    }


def analyze_image_with_replicate(image_path: Path, prompt: str = None) -> dict:
    """–ê–Ω–∞–ª—ñ–∑—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Replicate vision model —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
    if prompt is None:
        prompt = """Analyze this video frame for storyboard creation. Provide ONLY a valid JSON response with this exact structure:

{
  "scene_description": "Brief description of what's happening",
  "visual_elements": {
    "characters": ["person", "character"],
    "objects": ["object1", "object2"],
    "background": "background description",
    "lighting": "lighting description"
  },
  "composition": {
    "shot_type": "close-up",
    "camera_angle": "eye-level",
    "framing": "framing description"
  },
  "storyboard_suitability": {
    "score": 7,
    "reasoning": "explanation",
    "key_moments": ["moment1", "moment2"],
    "transition_potential": "good"
  },
  "production_notes": {
    "makeup_visible": true,
    "emotions": ["emotion1", "emotion2"],
    "action_level": "static",
    "technical_quality": "good"
  }
}

Respond with ONLY the JSON, no other text."""
    
    try:
        # –ß–∏—Ç–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ base64
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ data URL
        image_url = f"data:image/png;base64,{image_data}"
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ Replicate API –∑ –Ω–æ–≤–æ—é –º–æ–¥–µ–ª–ª—é
        output = replicate.run(
            "yorickvp/llava-13b:80537f9eead1a5bfa72d5ac6ea6414379be41d4d4f6679fd776e9535d1eb58bb",
            input={
                "image": image_url,
                "prompt": prompt,
                "max_tokens": 500,
                "temperature": 0.3
            }
        )
        
        # Replicate –ø–æ–≤–µ—Ä—Ç–∞—î generator, –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑—ñ–±—Ä–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if hasattr(output, '__iter__') and not isinstance(output, str):
            result = ''.join(str(item) for item in output)
        else:
            result = str(output)
        
        # –°–ø—Ä–æ–±—É—î–º–æ –ø–∞—Ä—Å–∏—Ç–∏ JSON, —è–∫—â–æ –Ω–µ –≤–¥–∞—î—Ç—å—Å—è - —Å—Ç–≤–æ—Ä–∏–º–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
        try:
            import json
            import re
            
            # –°–ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ JSON —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            json_match = re.search(r'\{.*\}', result.strip(), re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                parsed_result = json.loads(json_str)
                return parsed_result
            else:
                # –Ø–∫—â–æ JSON –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, —Å—Ç–≤–æ—Ä–∏–º–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∑ —Ç–µ–∫—Å—Ç—É
                return create_structured_analysis_from_text(result.strip())
        except json.JSONDecodeError:
            return create_structured_analysis_from_text(result.strip())
            
    except Exception as exc:
        return {
            "error": f"Analysis failed: {exc}",
            "scene_description": "Analysis unavailable",
            "storyboard_suitability": {"score": 0, "reasoning": "Analysis failed"}
        }


def save_analysis_to_file(analysis: dict, output_path: Path) -> None:
    """–ó–±–µ—Ä—ñ–≥–∞—î —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —É JSON —Ñ–∞–π–ª—ñ."""
    # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
    output_path.parent.mkdir(parents=True, exist_ok=True)
    import json
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)


def upload_image_to_notion(notion: Client, image_path: Path) -> str:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ Notion —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î URL."""
    try:
        # –î–ª—è —Ç–µ–ø–µ—Ä—ñ—à–Ω—å–æ–≥–æ —á–∞—Å—É –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ placeholder
        # –í –º–∞–π–±—É—Ç–Ω—å–æ–º—É –º–æ–∂–Ω–∞ —ñ–Ω—Ç–µ–≥—Ä—É–≤–∞—Ç–∏ –∑ –∑–æ–≤–Ω—ñ—à–Ω—ñ–º —Ö–æ—Å—Ç–∏–Ω–≥–æ–º –∑–æ–±—Ä–∞–∂–µ–Ω—å
        return f"üì∑ {image_path.name} (–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ)"
    except Exception as exc:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ Notion: {exc}", file=sys.stderr)
        return ""


def create_html_storyboard(
    video_name: str,
    screenshots: List[Path],
    analyses: List[Path],
    timepoints: List[TimePoint],
    output_path: Path
) -> None:
    """–°—Ç–≤–æ—Ä—é—î HTML —Ñ–∞–π–ª –∑ storyboard."""
    html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Storyboard: {video_name}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
            background: #f8f9fa;
        }}
        .header {{
            background: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }}
        .frame {{
            background: white;
            margin-bottom: 30px;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .frame-header {{
            background: #2563eb;
            color: white;
            padding: 15px 20px;
            font-weight: 600;
            font-size: 18px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }}
        .score-badge {{
            background: rgba(255,255,255,0.2);
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 14px;
        }}
        .frame-content {{
            padding: 20px;
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
        }}
        .screenshot {{
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin-bottom: 15px;
        }}
        .analysis {{
            background: #f0f9ff;
            border-left: 4px solid #0ea5e9;
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 8px 8px 0;
        }}
        .analysis-section {{
            margin-bottom: 15px;
        }}
        .analysis-title {{
            font-weight: 600;
            color: #1e40af;
            margin-bottom: 5px;
        }}
        .metadata {{
            color: #6b7280;
            font-size: 14px;
            margin-top: 10px;
        }}
        .divider {{
            height: 1px;
            background: #e5e7eb;
            margin: 20px 0;
        }}
        .score-high {{ color: #10b981; }}
        .score-medium {{ color: #f59e0b; }}
        .score-low {{ color: #ef4444; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>üé¨ Storyboard: {video_name}</h1>
        <p>AI-Generated Storyboard Analysis</p>
        <p>Frames: {len(screenshots)} | Created: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
"""

    for i, (screenshot, analysis, tp) in enumerate(zip(screenshots, analyses, timepoints), 1):
        time_label = format_timestamp(tp.seconds, precision='ms')
        
        # –ß–∏—Ç–∞—î–º–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
        analysis_data = {}
        score = 5
        if analysis and analysis.exists():
            try:
                import json
                with open(analysis, 'r', encoding='utf-8') as f:
                    analysis_data = json.load(f)
                score = analysis_data.get('storyboard_suitability', {}).get('score', 5)
            except:
                analysis_data = {"scene_description": "Analysis unavailable"}
        
        # –ö–æ–ø—ñ—é—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ exports –ø–∞–ø–∫—É –¥–ª—è HTML
        screenshot_name = f"frame_{i:03d}_{screenshot.name}"
        screenshot_dest = output_path.parent / screenshot_name
        import shutil
        shutil.copy2(screenshot, screenshot_dest)
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫–ª–∞—Å –¥–ª—è –æ—Ü—ñ–Ω–∫–∏
        score_class = "score-high" if score >= 7 else "score-medium" if score >= 4 else "score-low"
        
        html_content += f"""
    <div class="frame">
        <div class="frame-header">
            Frame {i} - {time_label}
            <span class="score-badge {score_class}">Score: {score}/10</span>
        </div>
        <div class="frame-content">
            <div>
                <img src="{screenshot_name}" alt="Frame {i}" class="screenshot">
                <div class="metadata">
                    Time: {time_label} | Frame: #{tp.frame_index} | Index: {tp.index}
                </div>
            </div>
            <div class="analysis">
"""
        
        # –î–æ–¥–∞—î–º–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
        if analysis_data:
            html_content += f"""
                <div class="analysis-section">
                    <div class="analysis-title">üìù Scene Description</div>
                    <div>{analysis_data.get('scene_description', 'N/A')}</div>
                </div>
"""
            
            if 'visual_elements' in analysis_data:
                ve = analysis_data['visual_elements']
                html_content += f"""
                <div class="analysis-section">
                    <div class="analysis-title">üëÅÔ∏è Visual Elements</div>
                    <div><strong>Characters:</strong> {', '.join(ve.get('characters', ['N/A']))}</div>
                    <div><strong>Objects:</strong> {', '.join(ve.get('objects', ['N/A']))}</div>
                    <div><strong>Background:</strong> {ve.get('background', 'N/A')}</div>
                    <div><strong>Lighting:</strong> {ve.get('lighting', 'N/A')}</div>
                </div>
"""
            
            if 'composition' in analysis_data:
                comp = analysis_data['composition']
                html_content += f"""
                <div class="analysis-section">
                    <div class="analysis-title">üé¨ Composition</div>
                    <div><strong>Shot Type:</strong> {comp.get('shot_type', 'N/A')}</div>
                    <div><strong>Camera Angle:</strong> {comp.get('camera_angle', 'N/A')}</div>
                    <div><strong>Framing:</strong> {comp.get('framing', 'N/A')}</div>
                </div>
"""
            
            if 'storyboard_suitability' in analysis_data:
                ss = analysis_data['storyboard_suitability']
                html_content += f"""
                <div class="analysis-section">
                    <div class="analysis-title">‚≠ê Storyboard Suitability</div>
                    <div><strong>Score:</strong> {ss.get('score', 'N/A')}/10</div>
                    <div><strong>Reasoning:</strong> {ss.get('reasoning', 'N/A')}</div>
                    <div><strong>Key Moments:</strong> {', '.join(ss.get('key_moments', ['N/A']))}</div>
                    <div><strong>Transition Potential:</strong> {ss.get('transition_potential', 'N/A')}</div>
                </div>
"""
        
        html_content += """
            </div>
        </div>
    </div>
"""
        
        if i < len(screenshots):
            html_content += '<div class="divider"></div>'

    html_content += """
</body>
</html>
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)


def create_editing_table(
    video_name: str,
    screenshots: List[Path],
    analyses: List[Path],
    timepoints: List[TimePoint],
    output_path: Path,
    transcript_data: dict = None
) -> None:
    """–°—Ç–≤–æ—Ä—é—î —Ç–∞–±–ª–∏—Ü—é –¥–ª—è –º–æ–Ω—Ç–∞–∂—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞–Ω–∞–ª—ñ–∑—É –∫–∞–¥—Ä—ñ–≤."""
    import json
    
    # –ó–±–∏—Ä–∞—î–º–æ –¥–∞–Ω—ñ –∑ –∞–Ω–∞–ª—ñ–∑—ñ–≤
    editing_data = []
    
    for i, (screenshot, analysis, tp) in enumerate(zip(screenshots, analyses, timepoints), 1):
        time_label = format_timestamp(tp.seconds, precision='ms')
        
        # –ß–∏—Ç–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑
        analysis_data = {}
        if analysis and analysis.exists():
            try:
                with open(analysis, 'r', encoding='utf-8') as f:
                    analysis_data = json.load(f)
            except:
                analysis_data = {"scene_description": "Analysis unavailable"}
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞–Ω–∞–ª—ñ–∑—É
        content_type = "Image"
        if analysis_data.get('production_notes', {}).get('action_level') in ['medium', 'high']:
            content_type = "Video"
        elif analysis_data.get('storyboard_suitability', {}).get('transition_potential', '').lower() in ['high', 'excellent']:
            content_type = "Montage"
        
        # –ì–µ–Ω–µ—Ä—É—î–º–æ –æ–ø–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
        scene_desc = analysis_data.get('scene_description', 'Scene analysis unavailable')
        visual_elements = analysis_data.get('visual_elements', {})
        characters = ', '.join(visual_elements.get('characters', ['character']))
        objects = ', '.join(visual_elements.get('objects', ['objects']))
        background = visual_elements.get('background', 'background')
        lighting = visual_elements.get('lighting', 'lighting')
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–µ—Ç–∞–ª—å–Ω–∏–π –æ–ø–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        generation_prompt = f"""Generate {content_type.lower()} content: {scene_desc}. 
Characters: {characters}. Objects: {objects}. 
Background: {background}. Lighting: {lighting}.
Maintain character identity and visual consistency."""
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ VO/Sound
        vo_sound = ""
        
        # –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ñ
        if transcript_data:
            vo_sound = get_vo_for_timestamp(transcript_data, tp.seconds)
        
        # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–Ω–∞–ª—ñ–∑
        if not vo_sound:
            emotions = analysis_data.get('production_notes', {}).get('emotions', [])
            if emotions:
                if 'happy' in emotions or 'excited' in emotions:
                    vo_sound = "Upbeat dialogue or music"
                elif 'serious' in emotions or 'focused' in emotions:
                    vo_sound = "Professional narration"
                elif 'dramatic' in emotions:
                    vo_sound = "Dramatic music or sound effects"
        
        editing_data.append({
            'step': i,
            'timestamp': time_label,
            'type': content_type,
            'description': generation_prompt,
            'vo_sound': vo_sound,
            'score': analysis_data.get('storyboard_suitability', {}).get('score', 5),
            'key_moments': analysis_data.get('storyboard_suitability', {}).get('key_moments', [])
        })
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ HTML —Ç–∞–±–ª–∏—Ü—é
    html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Editing Table: {video_name}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1600px;
            margin: 0 auto;
            padding: 20px;
            background: #f8f9fa;
        }}
        .header {{
            background: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }}
        .table-container {{
            background: white;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
        }}
        th {{
            background: #2563eb;
            color: white;
            padding: 15px 10px;
            text-align: left;
            font-weight: 600;
        }}
        td {{
            padding: 15px 10px;
            border-bottom: 1px solid #e5e7eb;
            vertical-align: top;
        }}
        tr:nth-child(even) {{
            background: #f9fafb;
        }}
        .step {{
            font-weight: 600;
            color: #2563eb;
        }}
        .type-image {{ color: #10b981; }}
        .type-video {{ color: #f59e0b; }}
        .type-montage {{ color: #8b5cf6; }}
        .description {{
            max-width: 400px;
            word-wrap: break-word;
        }}
        .score-high {{ color: #10b981; font-weight: 600; }}
        .score-medium {{ color: #f59e0b; font-weight: 600; }}
        .score-low {{ color: #ef4444; font-weight: 600; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>üé¨ Editing Table: {video_name}</h1>
        <p>AI-Generated Editing Guide for Generative Models</p>
        <p>Total Steps: {len(editing_data)} | Created: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="table-container">
        <table>
            <thead>
                <tr>
                    <th>Step</th>
                    <th>Timestamp</th>
                    <th>Type</th>
                    <th>Description</th>
                    <th>VO / Sound</th>
                    <th>Score</th>
                </tr>
            </thead>
            <tbody>
"""
    
    for item in editing_data:
        score_class = "score-high" if item['score'] >= 7 else "score-medium" if item['score'] >= 4 else "score-low"
        type_class = f"type-{item['type'].lower()}"
        
        html_content += f"""
                <tr>
                    <td class="step">{item['step']}</td>
                    <td>{item['timestamp']}</td>
                    <td class="{type_class}">{item['type']}</td>
                    <td class="description">{item['description']}</td>
                    <td>{item['vo_sound']}</td>
                    <td class="{score_class}">{item['score']}/10</td>
                </tr>
"""
    
    html_content += """
            </tbody>
        </table>
    </div>
</body>
</html>
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)


def create_notion_storyboard(
    notion: Client,
    parent_page_id: str,
    video_name: str,
    screenshots: List[Path],
    analyses: List[Path],
    timepoints: List[TimePoint]
) -> str:
    """–°—Ç–≤–æ—Ä—é—î Notion —Å—Ç–æ—Ä—ñ–Ω–∫—É –∑ storyboard."""
    try:
        # –°—Ç–≤–æ—Ä—é—î–º–æ –≥–æ–ª–æ–≤–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É storyboard
        page_title = f"Storyboard: {video_name}"
        
        page = notion.pages.create(
            parent={"page_id": parent_page_id},
            properties={
                "title": {
                    "title": [{"text": {"content": page_title}}]
                }
            }
        )
        
        page_id = page["id"]
        
        # –î–æ–¥–∞—î–º–æ –æ–ø–∏—Å
        notion.blocks.children.append(
            block_id=page_id,
            children=[
                {
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [
                            {
                                "type": "text",
                                "text": {
                                    "content": f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–∏–π storyboard –¥–ª—è –≤—ñ–¥–µ–æ {video_name}\n"
                                             f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–∞–¥—Ä—ñ–≤: {len(screenshots)}\n"
                                             f"–°—Ç–≤–æ—Ä–µ–Ω–æ: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                                }
                            }
                        ]
                    }
                }
            ]
        )
        
        # –î–æ–¥–∞—î–º–æ –∫–æ–∂–µ–Ω –∫–∞–¥—Ä
        for i, (screenshot, analysis, tp) in enumerate(zip(screenshots, analyses, timepoints), 1):
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            image_url = upload_image_to_notion(notion, screenshot)
            
            if not image_url:
                continue
            
            # –ß–∏—Ç–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑
            analysis_text = ""
            if analysis.exists():
                with open(analysis, 'r', encoding='utf-8') as f:
                    analysis_text = f.read().strip()
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –±–ª–æ–∫ –¥–ª—è –∫–∞–¥—Ä—É
            frame_blocks = [
                {
                    "object": "block",
                    "type": "heading_2",
                    "heading_2": {
                        "rich_text": [
                            {
                                "type": "text",
                                "text": {
                                    "content": f"–ö–∞–¥—Ä {i} - {format_timestamp(tp.seconds, precision='ms')}"
                                }
                            }
                        ]
                    }
                },
                {
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [
                            {
                                "type": "text",
                                "text": {"content": image_url}
                            }
                        ]
                    }
                }
            ]
            
            # –î–æ–¥–∞—î–º–æ –∞–Ω–∞–ª—ñ–∑ —è–∫—â–æ —î
            if analysis_text:
                frame_blocks.append({
                    "object": "block",
                    "type": "callout",
                    "callout": {
                        "rich_text": [
                            {
                                "type": "text",
                                "text": {"content": analysis_text}
                            }
                        ],
                        "icon": {"emoji": "ü§ñ"}
                    }
                })
            
            # –î–æ–¥–∞—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ
            frame_blocks.append({
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": {
                    "rich_text": [
                        {
                            "type": "text",
                            "text": {
                                "content": f"–ß–∞—Å: {format_timestamp(tp.seconds, precision='ms')} | "
                                         f"–ö–∞–¥—Ä: #{tp.frame_index} | "
                                         f"–Ü–Ω–¥–µ–∫—Å: {tp.index}"
                            }
                        }
                    ]
                }
            })
            
            # –î–æ–¥–∞—î–º–æ —Ä–æ–∑–¥—ñ–ª—é–≤–∞—á
            frame_blocks.append({
                "object": "block",
                "type": "divider",
                "divider": {}
            })
            
            # –î–æ–¥–∞—î–º–æ –±–ª–æ–∫–∏ –¥–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            notion.blocks.children.append(
                block_id=page_id,
                children=frame_blocks
            )
        
        return page_id
        
    except Exception as exc:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è Notion —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {exc}", file=sys.stderr)
        return ""


def estimate_total_frames(duration: float, interval: float) -> int:
    if interval <= 0:
        return 0
    return int(math.floor(duration / interval + 1e-9)) + 1


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="video-shots",
        description="–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–∫—Ä—ñ–Ω—à–æ—Ç—ñ–≤ –∑ –≤—ñ–¥–µ–æ –ø–æ —á–∞—Å–æ–≤–æ–º—É —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É",
    )
    parser.add_argument("video_path", help="–®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –≤—ñ–¥–µ–æ")
    parser.add_argument(
        "-n",
        "--interval",
        required=True,
        help="–ß–∞—Å–æ–≤–∏–π —ñ–Ω—Ç–µ—Ä–≤–∞–ª –º—ñ–∂ —Å–∫—Ä—ñ–Ω—à–æ—Ç–∞–º–∏ (–ø—Ä–∏–∫–ª–∞–¥–∏: 0.5s, 200ms, 00:00:03.5)",
    )
    parser.add_argument(
        "--format",
        choices=["png", "jpg"],
        default="png",
        help="–§–æ—Ä–º–∞—Ç –∑–æ–±—Ä–∞–∂–µ–Ω—å (–¥–µ—Ñ–æ–ª—Ç: png)",
    )
    parser.add_argument(
        "--outdir",
        type=str,
        default=None,
        help="–í–∏—Ö—ñ–¥–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è —Å–∫—Ä—ñ–Ω—à–æ—Ç—ñ–≤",
    )
    parser.add_argument(
        "--prefix",
        type=str,
        default="shot",
        help="–ü—Ä–µ—Ñ—ñ–∫—Å —ñ–º–µ–Ω —Ñ–∞–π–ª—ñ–≤ (–¥–µ—Ñ–æ–ª—Ç: shot)",
    )
    parser.add_argument(
        "--start",
        type=str,
        default="0",
        help="–ü–æ—á–∞—Ç–∫–æ–≤–∏–π —á–∞—Å (–¥–µ—Ñ–æ–ª—Ç: 0)",
    )
    parser.add_argument(
        "--end",
        type=str,
        default=None,
        help="–ö—ñ–Ω—Ü–µ–≤–∏–π —á–∞—Å (–¥–µ—Ñ–æ–ª—Ç: —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –≤—ñ–¥–µ–æ)",
    )
    parser.add_argument(
        "--rotate",
        type=int,
        choices=[0, 90, 180, 270],
        default=0,
        help="–ü—Ä–∏–º—É—Å–æ–≤–µ –æ–±–µ—Ä—Ç–∞–Ω–Ω—è –∫–∞–¥—Ä—ñ–≤ (0/90/180/270)",
    )
    parser.add_argument(
        "--pdf",
        type=str,
        default=None,
        help="–ó–±–µ—Ä–µ–≥—Ç–∏ —É—Å—ñ —Å–∫—Ä—ñ–Ω—à–æ—Ç–∏ —É PDF-—Ñ–∞–π–ª (–≤–∫–∞–∑–∞—Ç–∏ —à–ª—è—Ö)",
    )
    parser.add_argument(
        "--time-precision",
        choices=["auto", "ms", "sec"],
        default="auto",
        help="–§–æ—Ä–º–∞—Ç —á–∞—Å—É —É –ø—ñ–¥–ø–∏—Å—ñ (auto, ms, sec)",
    )
    parser.add_argument(
        "--max-frames",
        type=int,
        default=None,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–∫—Ä—ñ–Ω—à–æ—Ç—ñ–≤ (–¥–ª—è –∑–∞—Ö–∏—Å—Ç—É –≤—ñ–¥ –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–æ–≥–æ —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É)",
    )
    parser.add_argument(
        "--analyze",
        action="store_true",
        help="–ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∫–æ–∂–µ–Ω —Å–∫—Ä—ñ–Ω—à–æ—Ç –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é AI vision model",
    )
    parser.add_argument(
        "--analysis-prompt",
        type=str,
        default="–û–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ —â–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –Ω–∞ —Ü—å–æ–º—É –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ñ",
        help="–ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∑–æ–±—Ä–∞–∂–µ–Ω—å (–¥–µ—Ñ–æ–ª—Ç: '–û–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ —â–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –Ω–∞ —Ü—å–æ–º—É –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ñ')",
    )
    parser.add_argument(
        "--notion",
        type=str,
        default=None,
        help="–°—Ç–≤–æ—Ä–∏—Ç–∏ Notion —Å—Ç–æ—Ä—ñ–Ω–∫—É –∑ storyboard (–≤–∫–∞–∑–∞—Ç–∏ ID –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏)",
    )
    parser.add_argument(
        "--html",
        type=str,
        default=None,
        help="–°—Ç–≤–æ—Ä–∏—Ç–∏ HTML —Ñ–∞–π–ª –∑ storyboard (–≤–∫–∞–∑–∞—Ç–∏ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É)",
    )
    parser.add_argument(
        "--editing-table",
        type=str,
        default=None,
        help="–°—Ç–≤–æ—Ä–∏—Ç–∏ —Ç–∞–±–ª–∏—Ü—é –¥–ª—è –º–æ–Ω—Ç–∞–∂—É (–≤–∫–∞–∑–∞—Ç–∏ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É)",
    )
    parser.add_argument(
        "--audio-transcript",
        type=str,
        default=None,
        help="–®–ª—è—Ö –¥–æ –∞—É–¥—ñ–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—É –¥–ª—è VO/Sound",
    )
    return parser


def resolve_output_dir(base_dir: Path, video_path: Path, custom_outdir: str | None) -> Path:
    if custom_outdir:
        out_dir = Path(custom_outdir)
    else:
        base_name = video_path.stem
        # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–∞–ø–∫—É –∑ timestamp –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å–µ—Å—ñ—ó
        timestamp = __import__('datetime').datetime.now().strftime('%Y%m%d_%H%M%S')
        out_dir = base_dir / f"{base_name}_{timestamp}"
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –æ—Å–Ω–æ–≤–Ω—É –ø–∞–ø–∫—É —Ç–∞ –ø—ñ–¥–ø–∞–ø–∫–∏
    out_dir.mkdir(parents=True, exist_ok=True)
    (out_dir / "screenshots").mkdir(exist_ok=True)
    (out_dir / "analysis").mkdir(exist_ok=True)
    (out_dir / "exports").mkdir(exist_ok=True)
    
    return out_dir


def main(argv: Sequence[str] | None = None) -> int:
    parser = build_arg_parser()
    args = parser.parse_args(argv)

    video_path = Path(args.video_path)
    if not video_path.exists():
        parser.error(f"–§–∞–π–ª –≤—ñ–¥–µ–æ '{video_path}' –Ω–µ —ñ—Å–Ω—É—î")

    try:
        interval_seconds = parse_time_value(args.interval)
    except VideoShotsError as exc:
        parser.error(str(exc))

    try:
        start_seconds = parse_time_value(args.start, allow_zero=True)
    except VideoShotsError as exc:
        parser.error(f"–ü–æ–º–∏–ª–∫–∞ —É --start: {exc}")

    end_seconds = None
    if args.end is not None:
        try:
            end_seconds = parse_time_value(args.end, allow_zero=True)
        except VideoShotsError as exc:
            parser.error(f"–ü–æ–º–∏–ª–∫–∞ —É --end: {exc}")

    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        parser.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –≤—ñ–¥–µ–æ '{video_path}'")

    fps = cap.get(cv2.CAP_PROP_FPS)
    if not fps or fps <= 0:
        cap.release()
        parser.error("–ù–µ–º–æ–∂–ª–∏–≤–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ FPS –≤—ñ–¥–µ–æ")

    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if frame_count <= 0:
        cap.release()
        parser.error("–ù–µ–º–æ–∂–ª–∏–≤–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–∞–¥—Ä—ñ–≤")

    duration = frame_count / fps
    if end_seconds is None:
        end_seconds = duration

    if start_seconds > duration:
        cap.release()
        parser.error("–ü–æ—á–∞—Ç–∫–æ–≤–∏–π —á–∞—Å –ø–µ—Ä–µ–≤–∏—â—É—î —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –≤—ñ–¥–µ–æ")

    estimated_total = estimate_total_frames(end_seconds - start_seconds, interval_seconds)
    if args.max_frames is not None and estimated_total > args.max_frames:
        cap.release()
        parser.error(
            f"–û—á—ñ–∫—É–≤–∞–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–∞–¥—Ä—ñ–≤ ({estimated_total}) –ø–µ—Ä–µ–≤–∏—â—É—î --max-frames"
        )

    if estimated_total > 5000:
        print(
            f"‚ö†Ô∏è –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: –±—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ {estimated_total} —Å–∫—Ä—ñ–Ω—ñ–≤."
            " –†–æ–∑–≥–ª—è–Ω—å—Ç–µ –æ–±–º–µ–∂–µ–Ω–Ω—è --start/--end –∞–±–æ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è —ñ–Ω—Ç–µ—Ä–≤–∞–ª—É.",
            file=sys.stderr,
        )

    theoretical_min_step = 1.0 / fps
    if interval_seconds < theoretical_min_step:
        print(
            f"‚ÑπÔ∏è –Ü–Ω—Ç–µ—Ä–≤–∞–ª {interval_seconds:.6f}s –º–µ–Ω—à–∏–π –∑–∞ 1/FPS ({theoretical_min_step:.6f}s)."
            " –ú–æ–∂–ª–∏–≤–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è –∫–∞–¥—Ä—ñ–≤.",
            file=sys.stderr,
        )

    try:
        timepoints = collect_timepoints(start_seconds, end_seconds, interval_seconds, fps, frame_count)
    except VideoShotsError as exc:
        cap.release()
        parser.error(str(exc))

    output_dir = resolve_output_dir(Path.cwd() / "output", video_path, args.outdir)

    actual_precision = args.time_precision
    if actual_precision == "auto":
        actual_precision = "ms" if interval_seconds < 1.0 else "sec"

    image_paths: List[Path] = []
    analysis_paths: List[Path] = []
    duplicates = 0
    previous_frame_idx = None

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å Replicate API —Ç–æ–∫–µ–Ω–∞
    if args.analyze:
        if not os.getenv("REPLICATE_API_TOKEN"):
            print("‚ö†Ô∏è –î–ª—è –∞–Ω–∞–ª—ñ–∑—É –∑–æ–±—Ä–∞–∂–µ–Ω—å –ø–æ—Ç—Ä—ñ–±–µ–Ω REPLICATE_API_TOKEN –≤ –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞", file=sys.stderr)
            print("   –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: export REPLICATE_API_TOKEN=your_token_here", file=sys.stderr)
            args.analyze = False

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å Notion API —Ç–æ–∫–µ–Ω–∞
    notion = None
    if args.notion:
        notion_token = os.getenv("NOTION_API_TOKEN")
        if not notion_token:
            print("‚ö†Ô∏è –î–ª—è Notion —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó –ø–æ—Ç—Ä—ñ–±–µ–Ω NOTION_API_TOKEN –≤ –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞", file=sys.stderr)
            print("   –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: export NOTION_API_TOKEN=your_token_here", file=sys.stderr)
            args.notion = None
        else:
            try:
                notion = Client(auth=notion_token)
                # –¢–µ—Å—Ç—É—î–º–æ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è
                notion.users.me()
                print("‚úÖ Notion API –ø—ñ–¥–∫–ª—é—á–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
            except Exception as exc:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Notion: {exc}", file=sys.stderr)
                args.notion = None

    for tp in timepoints:
        if previous_frame_idx is not None and tp.frame_index == previous_frame_idx:
            duplicates += 1
        previous_frame_idx = tp.frame_index

        cap.set(cv2.CAP_PROP_POS_FRAMES, tp.frame_index)
        ok, frame = cap.read()
        if not ok or frame is None:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –∫–∞–¥—Ä #{tp.frame_index}: –Ω–µ –≤–¥–∞–ª–æ—Å—è –∑—á–∏—Ç–∞—Ç–∏", file=sys.stderr)
            continue

        frame = apply_rotation(frame, args.rotate)

        time_label = format_timestamp(tp.seconds, precision=actual_precision)
        label_lines = [f"Time {time_label}"]
        framed = render_label_panel(frame, label_lines)

        time_tag = format_timestamp_for_name(tp.seconds, precision=actual_precision)
        filename = f"{args.prefix}_{tp.index:05d}_f{tp.frame_index:05d}_t{time_tag}.{args.format}"
        output_path = output_dir / "screenshots" / filename
        save_frame_image(framed, output_path, args.format)
        image_paths.append(output_path)

        # –ê–Ω–∞–ª—ñ–∑ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —è–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ
        analysis_path = None
        if args.analyze:
            print(f"üîç –ê–Ω–∞–ª—ñ–∑—É—é –∫–∞–¥—Ä {tp.index}/{len(timepoints)}...", end=" ", flush=True)
            analysis_data = analyze_image_with_replicate(output_path, args.analysis_prompt)
            analysis_filename = f"{args.prefix}_{tp.index:05d}_f{tp.frame_index:05d}_t{time_tag}_analysis.json"
            analysis_path = output_dir / "analysis" / analysis_filename
            save_analysis_to_file(analysis_data, analysis_path)
            scene_desc = analysis_data.get('scene_description', 'Analysis unavailable')
            print(f"‚úÖ {scene_desc[:50]}...")
        
        analysis_paths.append(analysis_path or Path())

    cap.release()

    if args.pdf:
        if args.pdf.startswith('/') or ':' in args.pdf:
            # –ê–±—Å–æ–ª—é—Ç–Ω–∏–π —à–ª—è—Ö
            pdf_path = Path(args.pdf)
        else:
            # –í—ñ–¥–Ω–æ—Å–Ω–∏–π —à–ª—è—Ö - –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ exports –ø–∞–ø—Ü—ñ
            pdf_path = output_dir / "exports" / args.pdf
        try:
            combine_to_pdf(image_paths, analysis_paths, timepoints, pdf_path)
            print(f"PDF –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {pdf_path}")
        except Exception as exc:  # pylint: disable=broad-except
            print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ PDF: {exc}", file=sys.stderr)

    # –°—Ç–≤–æ—Ä—é—î–º–æ HTML —Ñ–∞–π–ª —è–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ
    if args.html:
        print("üåê –°—Ç–≤–æ—Ä—é—é HTML storyboard...", end=" ", flush=True)
        if args.html.startswith('/') or ':' in args.html:
            # –ê–±—Å–æ–ª—é—Ç–Ω–∏–π —à–ª—è—Ö
            html_path = Path(args.html)
        else:
            # –í—ñ–¥–Ω–æ—Å–Ω–∏–π —à–ª—è—Ö - –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ exports –ø–∞–ø—Ü—ñ
            html_path = output_dir / "exports" / args.html
        create_html_storyboard(
            video_path.stem,
            image_paths,
            analysis_paths,
            timepoints,
            html_path
        )
        print(f"‚úÖ HTML storyboard –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {html_path}")

    # –ü–∞—Ä—Å–∏–º–æ –∞—É–¥—ñ–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —è–∫—â–æ –≤–∫–∞–∑–∞–Ω–æ
    transcript_data = {}
    if args.audio_transcript:
        print("üéµ –ü–∞—Ä—Å—É—é –∞—É–¥—ñ–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç...", end=" ", flush=True)
        transcript_data = parse_audio_transcript(Path(args.audio_transcript))
        print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(transcript_data)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤")

    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞–±–ª–∏—Ü—é –º–æ–Ω—Ç–∞–∂—É —è–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ
    if args.editing_table:
        print("üìä –°—Ç–≤–æ—Ä—é—é —Ç–∞–±–ª–∏—Ü—é –º–æ–Ω—Ç–∞–∂—É...", end=" ", flush=True)
        if args.editing_table.startswith('/') or ':' in args.editing_table:
            # –ê–±—Å–æ–ª—é—Ç–Ω–∏–π —à–ª—è—Ö
            table_path = Path(args.editing_table)
        else:
            # –í—ñ–¥–Ω–æ—Å–Ω–∏–π —à–ª—è—Ö - –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ exports –ø–∞–ø—Ü—ñ
            table_path = output_dir / "exports" / args.editing_table
        create_editing_table(
            video_path.stem,
            image_paths,
            analysis_paths,
            timepoints,
            table_path,
            transcript_data
        )
        print(f"‚úÖ –¢–∞–±–ª–∏—Ü—è –º–æ–Ω—Ç–∞–∂—É –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {table_path}")

    # –°—Ç–≤–æ—Ä—é—î–º–æ Notion —Å—Ç–æ—Ä—ñ–Ω–∫—É —è–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ
    if args.notion and notion:
        print("üìù –°—Ç–≤–æ—Ä—é—é Notion —Å—Ç–æ—Ä—ñ–Ω–∫—É...", end=" ", flush=True)
        notion_page_id = create_notion_storyboard(
            notion, 
            args.notion, 
            video_path.stem, 
            image_paths, 
            analysis_paths, 
            timepoints
        )
        if notion_page_id:
            print(f"‚úÖ Notion —Å—Ç–æ—Ä—ñ–Ω–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–∞: https://notion.so/{notion_page_id.replace('-', '')}")
        else:
            print("‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è Notion —Å—Ç–æ—Ä—ñ–Ω–∫–∏")

    print(f"–ì–æ—Ç–æ–≤–æ. –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(image_paths)} —Ñ–∞–π–ª—ñ–≤ —É '{output_dir}'.")
    if duplicates:
        print(f"‚ÑπÔ∏è {duplicates} —Ç–∞–π–º—à—Ç–∞–º–ø—ñ–≤ –≤–∫–∞–∑–∞–ª–∏ –Ω–∞ –æ–¥–Ω–∞–∫–æ–≤–∏–π –∫–∞–¥—Ä (–æ–±–º–µ–∂–µ–Ω–Ω—è FPS).")

    return 0


if __name__ == "__main__":
    try:
        raise SystemExit(main())
    except VideoShotsError as exc:
        print(f"–ü–æ–º–∏–ª–∫–∞: {exc}", file=sys.stderr)
        raise SystemExit(1) from exc

